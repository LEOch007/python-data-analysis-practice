{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIT 5800 Introduction to Big Data\n",
    "## Spring 2020\n",
    "### Assignment 2 - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "In this assignment, you will have an opportunity to:\n",
    "<ul>\n",
    "<li>apply data pre-processing tecniques that you learned in the class to a problem using Spark</li>\n",
    "<li>apply machine learning techniques that you learned in the class to a problem using Spark</li>\n",
    "</ul>\n",
    "\n",
    "<br/>\n",
    "To get started on this assignment, you need to download the given dataset and read the description carefully written on this page. Please note that all implementation of your program should be done with Python.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intended Learning Outcomes\n",
    "\n",
    "- Upon completion of this assignment, you should be able to:\n",
    "<ol>\n",
    "    <li>Demonstrate your understanding on how to pre-process data using the algorithms / techniques as described in the class.</li>\n",
    "    <li>Demonstrate your understanding on how to do prediction using the machine learning algorithms / techniques as described in the class.</li>\n",
    "    <li>Using PySpark to construct Python program to pre-process data, performing machine learning from the training data and do data classification for the testing set.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset contains daily weather observations from numerous Australian weather stations.\n",
    "The problem is to predict whether or not it will rain tomorrow by training a binary classification model on target RainTomorrow\n",
    "The target variable RainTomorrow means: Will it rain the next day? Yes or No.\n",
    "\n",
    "Note: You should exclude the variable Risk-MM when training a binary classification model. Not excluding it will leak the answers to your model and reduce its predictability. Read more about it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Note: The suggested functions below are for reference only. You can use any functions from PySpark.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Installing PySpark\n",
    "### Step 0.1 : Install Java\n",
    "#### Step 0.1.1: Check Java Version\n",
    "\n",
    "In command prompt:\n",
    "<pre>java -version</pre>\n",
    "\n",
    "Note: PySpark requires Java version 7 or later.\n",
    "\n",
    "#### Step 0.1.2\n",
    "Install java from the official download website.\n",
    "<url> https://www.oracle.com/java/technologies/javase-jdk8-downloads.html </url>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.2: Install Apache Spark on Windows\n",
    "<ol>\n",
    "    <li>Go to the <a href=\"http://spark.apache.org/downloads.html\">Spark download</a>.</li>\n",
    "    <li>Select the latest stable release (2.4.5 as of May-2020) of Spark for \"Choose a Spark release\".</li>\n",
    "    <li>Select a version that is pre-built for the latest version of Hadoop such as Pre-built for Hadoop 2.7 and later\n",
    "        for \"Choose a package type\"</li>\n",
    "    <li>Click the link next to Download Spark to download the spark-2.4.5-bin-hadoop2.7.tgz</li>\n",
    "    <li>Extract the files from the downloaded zip file using winzip or equivalent (right click on the extracted file and click extract here).</li>\n",
    "    <li>Make sure that the folder path and the folder name containing Spark files do not contain any spaces.</li>\n",
    "    <li>Create a folder called \"spark\" on your desktop and unzip the file that you downloaded as a folder called spark-2.4.5-bin-hadoop2.7. So, all Spark files will be in a folder called C:\\Users\\[your_user_name]\\Desktop\\Spark\\spark-2.4.0-bin-hadoop2.7. This will be referred as SPARK_HOME.</li>\n",
    "    <li>To test if your installation was successful, open Anaconda Prompt, change to SPARK_HOME directory and type bin\\pyspark. This should start the PySpark shell which can be used to interactively work with Spark. </li>\n",
    "    <li>Create a system environment variable in Windows called SPARK_HOME that points to the SPARK_HOME folder path.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.2: Install Apache Spark on Mac\n",
    "\n",
    "You can use Homebrew to install Apache Spark.\n",
    "<ol>\n",
    "    <li>Install Homebrew using the following command in your terminal:<br/>\n",
    "        <pre>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"</pre></li>\n",
    "    <li>Install Spark with Homebrew\n",
    "         <ol>\n",
    "         <li>In your terminal, type: <br />\n",
    "         <pre>brew install apache-spark</pre></li>\n",
    "         <li>You can check the version of spark: <br />\n",
    "         <pre>pyspark â€“version</pre></li>\n",
    "         </ol>\n",
    "    </li>\n",
    "    <li>You may need to install PySpark by:<br />\n",
    "    <pre>pip install pyspark</pre>\n",
    "    </li>\n",
    "    <li>To know where spark is installed: <br/>\n",
    "    <pre>brew info apache-spark</pre>\n",
    "    </li>\n",
    "    <li>Set the environment variables:<br />\n",
    "    <pre>export SPARK_HOME=\"[your_path]/ibexec/\"</pre>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.3 Using Pyspark in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findspark is a library that automatically sets up the development environment to import Apache Spark library.\n",
    "To install findspark, run the following in your shell:<br />\n",
    "<pre>pip install findspark</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing data and exploring the features (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1\n",
    "Read the csv file 'weatherAUS.csv' using \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.SparkSession\">SparkSession.read()</a> to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2\n",
    "Use show() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to print out the schema of the dataframe created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3\n",
    "Use printSchema() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to print out the schema of the dataframe created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the features above have a wrong datatype in the schema? What should the correct datatypes be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>(Write your answer here)</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert the data types of features (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1\n",
    "Import data types from \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#module-pyspark.sql.types\">pyspark.sql.types</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2\n",
    "Convert the datatype of features using:\n",
    "<ul>\n",
    "<li>withColumn() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>, and</li>\n",
    "<li>cast() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.Column\">pyspark.sql.Column</a>\n",
    "to convert the features (columns) into appropriate types \n",
    "(<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#module-pyspark.sql.types\">pyspark.sql.types</a>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:Exploring missing values (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 \n",
    "<ul>\n",
    "<li>Count the missing values in features using:\n",
    "<ul>\n",
    "<li>isNull() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.Column\">pyspark.sql.Column</a>\n",
    "to see whether there are missing values in a certain feature (column).</li>\n",
    "<li>filter() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
    "to filter rows given a condition, and</li>\n",
    "<li>count() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
    "to count the number of rows in a dataframe</li>\n",
    "</ul>\n",
    "<li>Print the features with missing values and their corresponding number of rows with missing values</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2\n",
    "Some features have values of \"NA\", which should be regarded as missing values.</br>\n",
    "Print those features with values of \"NA\" and their corresponding number of rows with values \"NA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Drop the feature, RISK_MM (2 points)\n",
    "As described in the dataset description, we will need to drop the feature, RISK_MM, using drop() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Processing the date feature (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1\n",
    "Import the to_date(), year(), month(), dayofmonth()\n",
    "from\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2\n",
    "Use the to_date() function to convert the <strong>Date</strong> attribute to datetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3\n",
    "Extract the <strong>year</strong>, <strong>month</strong>, <strong>day</strong> attributes of the converted <strong>Date</strong> attribute using year(), month() & dayofmonth()\n",
    "and create the corresponding new features <strong>Year</strong>, <strong>Month</strong> and <strong>Day</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4\n",
    "Drop the original <strong>Date</strong> feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Handling missing values of the categorical features (8 marks)\n",
    "### Step 6.1 Find the most frequent value for categorical features with \"NA\" values \n",
    "Use groupBy(), count() & show() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to list the distinct values of a feature and count the corresponding number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical features with missing values (\"NA\"), what is the most frequent value of each of the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">(Write your answer here)</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2\n",
    "Import when(), lit() from\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3\n",
    "For each of the categorical features with missing values (\"NA\"), replace \"NA\" with the corresponding most frequent value of each of the features using when(), lit(), otherwise()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 Handling missing values of the numerical features (10 marks)\n",
    "### Step 7.1 \n",
    "Print the list of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement(s) here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2\n",
    "Import Imputer from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.3\n",
    "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer\">pyspark.ml.feature.imputer</a> to fill in the missing values of the numerical features with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Transform the features (10 marks)\n",
    "### Step 8.1\n",
    "Import skewness from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.2\n",
    "We can get the skewness using skewness() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module.\n",
    "\n",
    "Find the features which are with skewness values larger than 0.75, and print the features together with their skewness values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.3\n",
    "Import log1p from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.4\n",
    "\n",
    "Apply log transformation on those features with skewness values larger than 0.75 using log1p() from \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 Converting Categorial features  (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.1 List categorical features\n",
    "Get and print the list of categorial features (exclude \"RainTomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.2 Convert categorical features into dummy/indicator features\n",
    "#### Step 9.2.1\n",
    "Import <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer\">StringIndex</a> and <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoderEstimator\">OneHotEncoderEstimator</a> from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9.2.2\n",
    "Using StringIndexer to convert categorical values into category indices for each of the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9.2.3\n",
    "Using OneHotEncoderEstimator to map a column of category indices to a column of binary vectors for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Training the Regression model  (20 points)\n",
    "\n",
    "### Step 10.1: Creating the feature vector\n",
    "\n",
    "#### Step 10.1.1\n",
    "Import VectorAssembler from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.1.2\n",
    "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\">VectorAssembler<a/>, a feature transformer, to merge the columns created in step 9.2.3 and the columns of numerical features into a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.2 \n",
    "Is there any other transformation on the dataframe needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.3 Split the data into training data and testing data\n",
    "\n",
    "Using randomSplit of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
    "to randomly splits the DataFrame with the ratio of 0.7 and 0.3 into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.4 Build the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.1\n",
    "Import LogisticRegression from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification\">pyspark.ml.classification</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.2\n",
    "<ol>\n",
    "<li>Initialize a Logistic Regression model by <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\">LogisticRegression()</a> function using the feature vector generated in Step 10.1.2.</li>\n",
    "<li>Use fit() function to train the logistic regression model using the training feature data.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.3\n",
    "<ul>\n",
    "    <li>Gets summary of model trained on the training set. </li>\n",
    "    <li>Print the accuracy, objective history, total iterations of the trained model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.4\n",
    "Predict the target values for the testing feature data using the transform() function.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.5\n",
    "Use show() to list the top 5 rows of results in Step 10.4.4, show only \"prediction\", \"RainTomorrow\" and the featuresCol specified in the LogisticRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.5 Evaluate the results\n",
    "#### Step 10.5.1 \n",
    "Import MulticlassClassificationEvaluator from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation\">pyspark.ml.evaluation</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.5.2\n",
    "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator\">pyspark.ml.evaluation.MulticlassClassificationEvaluator</a> to evaluate the predictions from Step 10.4.4.\n",
    "Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Pipeline (10 marks)\n",
    "\n",
    "### Step 11.1\n",
    "Import <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline\">Pipeline</a> from \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml\">pyspark.ml</a> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11.2\n",
    "\n",
    "Rewrite Steps 9.2 to 10.5:\n",
    "<ol>\n",
    "    <li>Configure an ML pipeline which consists of the StringIndexer, OneHotEncoderEstimator, VectorAssembler, LogisticRegression</li>\n",
    "    <li>Fit the pipeline to the training data.</li>\n",
    "    <li>Make predictions on the testing data.</li>\n",
    "    <li>Evaluate the prediction results as in step 10.5.</li>\n",
    "</ol>\n",
    "\n",
    "Note: \n",
    "<ul>\n",
    "    <li>It is fine to have the steps 9.2 to 10.5 slightly rearranged in this step.</li>\n",
    "    <li>Hence, the evaluation results may be slightly different.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12 Submission\n",
    "Submit your jupyter notebook (.ipynb) to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>The end of HW2</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
