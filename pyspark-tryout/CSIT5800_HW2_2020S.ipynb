{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "CSIT5800_HW2_2020S.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOSRD9623Zr_",
        "colab_type": "text"
      },
      "source": [
        "# CSIT 5800 Introduction to Big Data\n",
        "## Spring 2020\n",
        "### Assignment 2 - PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9w2xNzD3ZsB",
        "colab_type": "text"
      },
      "source": [
        "### Description\n",
        "In this assignment, you will have an opportunity to:\n",
        "<ul>\n",
        "<li>apply data pre-processing tecniques that you learned in the class to a problem using Spark</li>\n",
        "<li>apply machine learning techniques that you learned in the class to a problem using Spark</li>\n",
        "</ul>\n",
        "\n",
        "<br/>\n",
        "To get started on this assignment, you need to download the given dataset and read the description carefully written on this page. Please note that all implementation of your program should be done with Python.\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjiMfUyM3ZsC",
        "colab_type": "text"
      },
      "source": [
        "### Intended Learning Outcomes\n",
        "\n",
        "- Upon completion of this assignment, you should be able to:\n",
        "<ol>\n",
        "    <li>Demonstrate your understanding on how to pre-process data using the algorithms / techniques as described in the class.</li>\n",
        "    <li>Demonstrate your understanding on how to do prediction using the machine learning algorithms / techniques as described in the class.</li>\n",
        "    <li>Using PySpark to construct Python program to pre-process data, performing machine learning from the training data and do data classification for the testing set.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8kr68le3ZsC",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "The dataset contains daily weather observations from numerous Australian weather stations.\n",
        "The problem is to predict whether or not it will rain tomorrow by training a binary classification model on target RainTomorrow\n",
        "The target variable RainTomorrow means: Will it rain the next day? Yes or No.\n",
        "\n",
        "Note: You should exclude the variable Risk-MM when training a binary classification model. Not excluding it will leak the answers to your model and reduce its predictability. Read more about it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPMhSjn63ZsD",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"red\">Note: The suggested functions below are for reference only. You can use any functions from PySpark.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0uAPnhPecbF",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Installing PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qeuiq0heEmU",
        "colab_type": "code",
        "outputId": "5af055f3-015a-41a2-fbcc-c10a9dac3a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irki927I3ZsE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Step 0.1 : Install Java\n",
        "#### Step 0.1.1: Check Java Version\n",
        "\n",
        "In command prompt:\n",
        "<pre>java -version</pre>\n",
        "\n",
        "Note: PySpark requires Java version 7 or later.\n",
        "\n",
        "#### Step 0.1.2\n",
        "Install java from the official download website.\n",
        "<url> https://www.oracle.com/java/technologies/javase-jdk8-downloads.html </url>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdU7B3Ag3ZsF",
        "colab_type": "text"
      },
      "source": [
        "### Step 0.2: Install Apache Spark on Windows\n",
        "<ol>\n",
        "    <li>Go to the <a href=\"http://spark.apache.org/downloads.html\">Spark download</a>.</li>\n",
        "    <li>Select the latest stable release (2.4.5 as of May-2020) of Spark for \"Choose a Spark release\".</li>\n",
        "    <li>Select a version that is pre-built for the latest version of Hadoop such as Pre-built for Hadoop 2.7 and later\n",
        "        for \"Choose a package type\"</li>\n",
        "    <li>Click the link next to Download Spark to download the spark-2.4.5-bin-hadoop2.7.tgz</li>\n",
        "    <li>Extract the files from the downloaded zip file using winzip or equivalent (right click on the extracted file and click extract here).</li>\n",
        "    <li>Make sure that the folder path and the folder name containing Spark files do not contain any spaces.</li>\n",
        "    <li>Create a folder called \"spark\" on your desktop and unzip the file that you downloaded as a folder called spark-2.4.5-bin-hadoop2.7. So, all Spark files will be in a folder called C:\\Users\\[your_user_name]\\Desktop\\Spark\\spark-2.4.0-bin-hadoop2.7. This will be referred as SPARK_HOME.</li>\n",
        "    <li>To test if your installation was successful, open Anaconda Prompt, change to SPARK_HOME directory and type bin\\pyspark. This should start the PySpark shell which can be used to interactively work with Spark. </li>\n",
        "    <li>Create a system environment variable in Windows called SPARK_HOME that points to the SPARK_HOME folder path.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcXeltnI3ZsF",
        "colab_type": "text"
      },
      "source": [
        "### Step 0.2: Install Apache Spark on Mac\n",
        "\n",
        "You can use Homebrew to install Apache Spark.\n",
        "<ol>\n",
        "    <li>Install Homebrew using the following command in your terminal:<br/>\n",
        "        <pre>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"</pre></li>\n",
        "    <li>Install Spark with Homebrew\n",
        "         <ol>\n",
        "         <li>In your terminal, type: <br />\n",
        "         <pre>brew install apache-spark</pre></li>\n",
        "         <li>You can check the version of spark: <br />\n",
        "         <pre>pyspark â€“version</pre></li>\n",
        "         </ol>\n",
        "    </li>\n",
        "    <li>You may need to install PySpark by:<br />\n",
        "    <pre>pip install pyspark</pre>\n",
        "    </li>\n",
        "    <li>To know where spark is installed: <br/>\n",
        "    <pre>brew info apache-spark</pre>\n",
        "    </li>\n",
        "    <li>Set the environment variables:<br />\n",
        "    <pre>export SPARK_HOME=\"[your_path]/ibexec/\"</pre>\n",
        "    </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkj1zjmj3ZsG",
        "colab_type": "text"
      },
      "source": [
        "### Step 0.3 Using Pyspark in Jupyter Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZy4kHyV3ZsH",
        "colab_type": "text"
      },
      "source": [
        "findspark is a library that automatically sets up the development environment to import Apache Spark library.\n",
        "To install findspark, run the following in your shell:<br />\n",
        "<pre>pip install findspark</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N6W8k9r3ZsH",
        "colab_type": "code",
        "outputId": "aa346b9f-8aab-42d3-e613-facd4d3a26e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-2.4.5-bin-hadoop2.7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYQkp2O3ZsM",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Importing data and exploring the features (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqPmPKXQ3ZsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tgj5npy3ZsQ",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.1\n",
        "Read the csv file 'weatherAUS.csv' using \n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.SparkSession\">SparkSession.read()</a> to create a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_EfyEDi3ZsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = spark.read.csv('/content/weatherAUS.csv',header=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95jF0PUW3ZsU",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.2\n",
        "Use show() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to print out the schema of the dataframe created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLAJKm6w3ZsV",
        "colab_type": "code",
        "outputId": "caf6b13e-13a0-4f11-ac8f-94bc19fe83fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "data.show()\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
            "|      Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow|\n",
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
            "|2008-12-01|  Albury|   13.4|   22.9|     0.6|         NA|      NA|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       8|      NA|   16.9|   21.8|       No|      0|          No|\n",
            "|2008-12-02|  Albury|    7.4|   25.1|       0|         NA|      NA|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|      NA|      NA|   17.2|   24.3|       No|      0|          No|\n",
            "|2008-12-03|  Albury|   12.9|   25.7|       0|         NA|      NA|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|      NA|       2|     21|   23.2|       No|      0|          No|\n",
            "|2008-12-04|  Albury|    9.2|     28|       0|         NA|      NA|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|      NA|      NA|   18.1|   26.5|       No|      1|          No|\n",
            "|2008-12-05|  Albury|   17.5|   32.3|       1|         NA|      NA|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|       1006|       7|       8|   17.8|   29.7|       No|    0.2|          No|\n",
            "|2008-12-06|  Albury|   14.6|   29.7|     0.2|         NA|      NA|        WNW|           56|         W|         W|          19|          24|         55|         23|     1009.2|     1005.4|      NA|      NA|   20.6|   28.9|       No|      0|          No|\n",
            "|2008-12-07|  Albury|   14.3|     25|       0|         NA|      NA|          W|           50|        SW|         W|          20|          24|         49|         19|     1009.6|     1008.2|       1|      NA|   18.1|   24.6|       No|      0|          No|\n",
            "|2008-12-08|  Albury|    7.7|   26.7|       0|         NA|      NA|          W|           35|       SSE|         W|           6|          17|         48|         19|     1013.4|     1010.1|      NA|      NA|   16.3|   25.5|       No|      0|          No|\n",
            "|2008-12-09|  Albury|    9.7|   31.9|       0|         NA|      NA|        NNW|           80|        SE|        NW|           7|          28|         42|          9|     1008.9|     1003.6|      NA|      NA|   18.3|   30.2|       No|    1.4|         Yes|\n",
            "|2008-12-10|  Albury|   13.1|   30.1|     1.4|         NA|      NA|          W|           28|         S|       SSE|          15|          11|         58|         27|       1007|     1005.7|      NA|      NA|   20.1|   28.2|      Yes|      0|          No|\n",
            "|2008-12-11|  Albury|   13.4|   30.4|       0|         NA|      NA|          N|           30|       SSE|       ESE|          17|           6|         48|         22|     1011.8|     1008.7|      NA|      NA|   20.4|   28.8|       No|    2.2|         Yes|\n",
            "|2008-12-12|  Albury|   15.9|   21.7|     2.2|         NA|      NA|        NNE|           31|        NE|       ENE|          15|          13|         89|         91|     1010.5|     1004.2|       8|       8|   15.9|     17|      Yes|   15.6|         Yes|\n",
            "|2008-12-13|  Albury|   15.9|   18.6|    15.6|         NA|      NA|          W|           61|       NNW|       NNW|          28|          28|         76|         93|      994.3|        993|       8|       8|   17.4|   15.8|      Yes|    3.6|         Yes|\n",
            "|2008-12-14|  Albury|   12.6|     21|     3.6|         NA|      NA|         SW|           44|         W|       SSW|          24|          20|         65|         43|     1001.2|     1001.8|      NA|       7|   15.8|   19.8|      Yes|      0|          No|\n",
            "|2008-12-16|  Albury|    9.8|   27.7|      NA|         NA|      NA|        WNW|           50|        NA|       WNW|          NA|          22|         50|         28|     1013.4|     1010.3|       0|      NA|   17.3|   26.2|       NA|      0|          No|\n",
            "|2008-12-17|  Albury|   14.1|   20.9|       0|         NA|      NA|        ENE|           22|       SSW|         E|          11|           9|         69|         82|     1012.2|     1010.4|       8|       1|   17.2|   18.1|       No|   16.8|         Yes|\n",
            "|2008-12-18|  Albury|   13.5|   22.9|    16.8|         NA|      NA|          W|           63|         N|       WNW|           6|          20|         80|         65|     1005.8|     1002.2|       8|       1|     18|   21.5|      Yes|   10.6|         Yes|\n",
            "|2008-12-19|  Albury|   11.2|   22.5|    10.6|         NA|      NA|        SSE|           43|       WSW|        SW|          24|          17|         47|         32|     1009.4|     1009.7|      NA|       2|   15.5|     21|      Yes|      0|          No|\n",
            "|2008-12-20|  Albury|    9.8|   25.6|       0|         NA|      NA|        SSE|           26|        SE|       NNW|          17|           6|         45|         26|     1019.2|     1017.1|      NA|      NA|   15.8|   23.2|       No|      0|          No|\n",
            "|2008-12-21|  Albury|   11.5|   29.3|       0|         NA|      NA|          S|           24|        SE|        SE|           9|           9|         56|         28|     1019.3|     1014.8|      NA|      NA|   19.1|   27.3|       No|      0|          No|\n",
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MzpOFuB3ZsZ",
        "colab_type": "text"
      },
      "source": [
        "### Step 1.3\n",
        "Use printSchema() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to print out the schema of the dataframe created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fvMGfuR3Zsa",
        "colab_type": "code",
        "outputId": "ee878e4e-4a62-4f73-f53b-c04f6b0c7b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "data.printSchema()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- MinTemp: string (nullable = true)\n",
            " |-- MaxTemp: string (nullable = true)\n",
            " |-- Rainfall: string (nullable = true)\n",
            " |-- Evaporation: string (nullable = true)\n",
            " |-- Sunshine: string (nullable = true)\n",
            " |-- WindGustDir: string (nullable = true)\n",
            " |-- WindGustSpeed: string (nullable = true)\n",
            " |-- WindDir9am: string (nullable = true)\n",
            " |-- WindDir3pm: string (nullable = true)\n",
            " |-- WindSpeed9am: string (nullable = true)\n",
            " |-- WindSpeed3pm: string (nullable = true)\n",
            " |-- Humidity9am: string (nullable = true)\n",
            " |-- Humidity3pm: string (nullable = true)\n",
            " |-- Pressure9am: string (nullable = true)\n",
            " |-- Pressure3pm: string (nullable = true)\n",
            " |-- Cloud9am: string (nullable = true)\n",
            " |-- Cloud3pm: string (nullable = true)\n",
            " |-- Temp9am: string (nullable = true)\n",
            " |-- Temp3pm: string (nullable = true)\n",
            " |-- RainToday: string (nullable = true)\n",
            " |-- RISK_MM: string (nullable = true)\n",
            " |-- RainTomorrow: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsQ1laZ-3Zsk",
        "colab_type": "text"
      },
      "source": [
        "Which of the features above have a wrong datatype in the schema? What should the correct datatypes be?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcnAgoX3Zsl",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>\n",
        "[MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am, Temp3pm, RISK_MM]\n",
        "</font>\n",
        "<br> The above features should be double datatype.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oj175Zf3Zsl",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Convert the data types of features (7 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TTkej6k3Zsm",
        "colab_type": "text"
      },
      "source": [
        "### Step 2.1\n",
        "Import data types from \n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#module-pyspark.sql.types\">pyspark.sql.types</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efWzWbz13Zsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgubX9fn3Zsq",
        "colab_type": "text"
      },
      "source": [
        "### Step 2.2\n",
        "Convert the datatype of features using:\n",
        "<ul>\n",
        "<li>withColumn() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>, and</li>\n",
        "<li>cast() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.Column\">pyspark.sql.Column</a>\n",
        "to convert the features (columns) into appropriate types \n",
        "(<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#module-pyspark.sql.types\">pyspark.sql.types</a>)</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t2_HY6k3Zsq",
        "colab_type": "code",
        "outputId": "7cc223c6-337f-4180-a712-10c0926897d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "str_features = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', \n",
        "                'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', \n",
        "                'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RISK_MM']\n",
        "\n",
        "for i in range(len(str_features)):\n",
        "  data = data.withColumn(str_features[i], data[str_features[i]].cast('double'))\n",
        "\n",
        "data.printSchema()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- MinTemp: double (nullable = true)\n",
            " |-- MaxTemp: double (nullable = true)\n",
            " |-- Rainfall: double (nullable = true)\n",
            " |-- Evaporation: double (nullable = true)\n",
            " |-- Sunshine: double (nullable = true)\n",
            " |-- WindGustDir: string (nullable = true)\n",
            " |-- WindGustSpeed: double (nullable = true)\n",
            " |-- WindDir9am: string (nullable = true)\n",
            " |-- WindDir3pm: string (nullable = true)\n",
            " |-- WindSpeed9am: double (nullable = true)\n",
            " |-- WindSpeed3pm: double (nullable = true)\n",
            " |-- Humidity9am: double (nullable = true)\n",
            " |-- Humidity3pm: double (nullable = true)\n",
            " |-- Pressure9am: double (nullable = true)\n",
            " |-- Pressure3pm: double (nullable = true)\n",
            " |-- Cloud9am: double (nullable = true)\n",
            " |-- Cloud3pm: double (nullable = true)\n",
            " |-- Temp9am: double (nullable = true)\n",
            " |-- Temp3pm: double (nullable = true)\n",
            " |-- RainToday: string (nullable = true)\n",
            " |-- RISK_MM: double (nullable = true)\n",
            " |-- RainTomorrow: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djCT6mPR3Zsv",
        "colab_type": "text"
      },
      "source": [
        "## Step 3:Exploring missing values (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efVW9P9G3Zsw",
        "colab_type": "text"
      },
      "source": [
        "### Step 3.1 \n",
        "<ul>\n",
        "<li>Count the missing values in features using:\n",
        "<ul>\n",
        "<li>isNull() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.Column\">pyspark.sql.Column</a>\n",
        "to see whether there are missing values in a certain feature (column).</li>\n",
        "<li>filter() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
        "to filter rows given a condition, and</li>\n",
        "<li>count() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
        "to count the number of rows in a dataframe</li>\n",
        "</ul>\n",
        "<li>Print the features with missing values and their corresponding number of rows with missing values</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfF59HS_3Zsy",
        "colab_type": "code",
        "outputId": "869a5fb4-c984-4aa9-e22d-886ffa97e3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for c in data.columns:\n",
        "  cnt = data.filter(data[c].isNull()).count()\n",
        "  if cnt !=0:\n",
        "    print(c + '\\t\\t' + str(cnt))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MinTemp\t\t637\n",
            "MaxTemp\t\t322\n",
            "Rainfall\t\t1406\n",
            "Evaporation\t\t60843\n",
            "Sunshine\t\t67816\n",
            "WindGustSpeed\t\t9270\n",
            "WindSpeed9am\t\t1348\n",
            "WindSpeed3pm\t\t2630\n",
            "Humidity9am\t\t1774\n",
            "Humidity3pm\t\t3610\n",
            "Pressure9am\t\t14014\n",
            "Pressure3pm\t\t13981\n",
            "Cloud9am\t\t53657\n",
            "Cloud3pm\t\t57094\n",
            "Temp9am\t\t904\n",
            "Temp3pm\t\t2726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPbOXJm53Zs2",
        "colab_type": "text"
      },
      "source": [
        "### Step 3.2\n",
        "Some features have values of \"NA\", which should be regarded as missing values.</br>\n",
        "Print those features with values of \"NA\" and their corresponding number of rows with values \"NA\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjG0N7JD3Zs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d85daeae-f7c6-4653-bdc9-1d8ea612371d"
      },
      "source": [
        "for c in data.columns:\n",
        "  cnt = data.filter(data[c].contains('NA')).count()\n",
        "  if cnt !=0:\n",
        "    print(c + '\\t\\t' + str(cnt))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WindGustDir\t\t9330\n",
            "WindDir9am\t\t10013\n",
            "WindDir3pm\t\t3778\n",
            "RainToday\t\t1406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BeAIli_3Zs6",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Drop the feature, RISK_MM (2 points)\n",
        "As described in the dataset description, we will need to drop the feature, RISK_MM, using drop() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz_pI3DU3Zs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "66e99019-d1db-42c5-f7fe-7b969446cbd1"
      },
      "source": [
        "data = data.drop('RISK_MM')\n",
        "data.printSchema()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- MinTemp: double (nullable = true)\n",
            " |-- MaxTemp: double (nullable = true)\n",
            " |-- Rainfall: double (nullable = true)\n",
            " |-- Evaporation: double (nullable = true)\n",
            " |-- Sunshine: double (nullable = true)\n",
            " |-- WindGustDir: string (nullable = true)\n",
            " |-- WindGustSpeed: double (nullable = true)\n",
            " |-- WindDir9am: string (nullable = true)\n",
            " |-- WindDir3pm: string (nullable = true)\n",
            " |-- WindSpeed9am: double (nullable = true)\n",
            " |-- WindSpeed3pm: double (nullable = true)\n",
            " |-- Humidity9am: double (nullable = true)\n",
            " |-- Humidity3pm: double (nullable = true)\n",
            " |-- Pressure9am: double (nullable = true)\n",
            " |-- Pressure3pm: double (nullable = true)\n",
            " |-- Cloud9am: double (nullable = true)\n",
            " |-- Cloud3pm: double (nullable = true)\n",
            " |-- Temp9am: double (nullable = true)\n",
            " |-- Temp3pm: double (nullable = true)\n",
            " |-- RainToday: string (nullable = true)\n",
            " |-- RainTomorrow: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgWinsLt3Zs9",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Processing the date feature (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg9odMP43Zs-",
        "colab_type": "text"
      },
      "source": [
        "### Step 5.1\n",
        "Import the to_date(), year(), month(), dayofmonth()\n",
        "from\n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc7Ih0I43Zs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w95mnWRY3ZtB",
        "colab_type": "text"
      },
      "source": [
        "### Step 5.2\n",
        "Use the to_date() function to convert the <strong>Date</strong> attribute to datetype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROFA5QC13ZtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.withColumn('Date',to_date(data['Date']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb7jaxlu3ZtH",
        "colab_type": "text"
      },
      "source": [
        "### Step 5.3\n",
        "Extract the <strong>year</strong>, <strong>month</strong>, <strong>day</strong> attributes of the converted <strong>Date</strong> attribute using year(), month() & dayofmonth()\n",
        "and create the corresponding new features <strong>Year</strong>, <strong>Month</strong> and <strong>Day</strong>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jRjZX0Z3ZtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.withColumn('Year',year(data['Date']))\n",
        "data = data.withColumn('Month',month(data['Date']))\n",
        "data = data.withColumn('Day',dayofmonth(data['Date']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_LUu7PH3ZtL",
        "colab_type": "text"
      },
      "source": [
        "### Step 5.4\n",
        "Drop the original <strong>Date</strong> feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQQxVntp3ZtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop('Date')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vcgITIi3ZtP",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Handling missing values of the categorical features (8 marks)\n",
        "### Step 6.1 Find the most frequent value for categorical features with \"NA\" values \n",
        "Use groupBy(), count() & show() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to list the distinct values of a feature and count the corresponding number of rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEWkGBbA3ZtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10e220c1-7943-43c5-9106-c055f949e972"
      },
      "source": [
        "cate_fea = ['WindGustDir', 'WindDir9am','WindDir3pm','RainToday']\n",
        "for c in cate_fea:\n",
        "  data.groupBy(c).count().show()\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|WindGustDir|count|\n",
            "+-----------+-----+\n",
            "|        SSE| 8993|\n",
            "|         SW| 8797|\n",
            "|         NW| 8003|\n",
            "|         NA| 9330|\n",
            "|          E| 9071|\n",
            "|        WSW| 8901|\n",
            "|        ENE| 7992|\n",
            "|         NE| 7060|\n",
            "|        NNW| 6561|\n",
            "|          N| 9033|\n",
            "|        SSW| 8610|\n",
            "|          W| 9780|\n",
            "|          S| 8949|\n",
            "|         SE| 9309|\n",
            "|        WNW| 8066|\n",
            "|        NNE| 6433|\n",
            "|        ESE| 7305|\n",
            "+-----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|WindDir9am|count|\n",
            "+----------+-----+\n",
            "|       SSE| 8966|\n",
            "|        SW| 8237|\n",
            "|        NW| 8552|\n",
            "|        NA|10013|\n",
            "|         E| 9024|\n",
            "|       WSW| 6843|\n",
            "|       ENE| 7735|\n",
            "|        NE| 7527|\n",
            "|       NNW| 7840|\n",
            "|         N|11393|\n",
            "|       SSW| 7448|\n",
            "|         W| 8260|\n",
            "|         S| 8493|\n",
            "|        SE| 9162|\n",
            "|       WNW| 7194|\n",
            "|       NNE| 7948|\n",
            "|       ESE| 7558|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|WindDir3pm|count|\n",
            "+----------+-----+\n",
            "|       SSE| 9142|\n",
            "|        NW| 8468|\n",
            "|        SW| 9182|\n",
            "|        NA| 3778|\n",
            "|         E| 8342|\n",
            "|       WSW| 9329|\n",
            "|       ENE| 7724|\n",
            "|        NE| 8164|\n",
            "|       NNW| 7733|\n",
            "|         N| 8667|\n",
            "|       SSW| 8010|\n",
            "|         W| 9911|\n",
            "|         S| 9598|\n",
            "|        SE|10663|\n",
            "|       WNW| 8656|\n",
            "|       NNE| 6444|\n",
            "|       ESE| 8382|\n",
            "+----------+-----+\n",
            "\n",
            "+---------+------+\n",
            "|RainToday| count|\n",
            "+---------+------+\n",
            "|       NA|  1406|\n",
            "|       No|109332|\n",
            "|      Yes| 31455|\n",
            "+---------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MgLsgoX3ZtS",
        "colab_type": "text"
      },
      "source": [
        "For the categorical features with missing values (\"NA\"), what is the most frequent value of each of the features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlhBCxSK3ZtS",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"red\">\n",
        "WindGustDir: W <br>\n",
        "WindDir9am: N <br>\n",
        "WindDir3pm: SE <br>\n",
        "RainToday: No <br>\n",
        "</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdv_8U9A3ZtT",
        "colab_type": "text"
      },
      "source": [
        "### Step 6.2\n",
        "Import when(), lit() from\n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBXZgrm43ZtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import when, lit\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOKtyoCa3ZtW",
        "colab_type": "text"
      },
      "source": [
        "### Step 6.3\n",
        "For each of the categorical features with missing values (\"NA\"), replace \"NA\" with the corresponding most frequent value of each of the features using when(), lit(), otherwise()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49vNYeOP3ZtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.withColumn('WindGustDir', when(data['WindGustDir'].contains('NA'), \n",
        "                                         lit('W')).otherwise(data['WindGustDir']))\n",
        "data = data.withColumn('WindDir9am', when(data['WindDir9am'].contains('NA'), \n",
        "                                         lit('N')).otherwise(data['WindDir9am']))\n",
        "data = data.withColumn('WindDir3pm', when(data['WindDir3pm'].contains('NA'), \n",
        "                                         lit('SE')).otherwise(data['WindDir3pm']))\n",
        "data = data.withColumn('RainToday', when(data['RainToday'].contains('NA'), \n",
        "                                         lit('No')).otherwise(data['RainToday']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXgMNdca3ZtZ",
        "colab_type": "text"
      },
      "source": [
        "## Step 7 Handling missing values of the numerical features (10 marks)\n",
        "### Step 7.1 \n",
        "Print the list of numerical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wM-iaWE3Zta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b22bbe28-f843-4500-d848-a5a4d896ba71"
      },
      "source": [
        "num_list = [item[0] for item in data.dtypes if (item[1].startswith('double'))|(item[1].startswith('int'))]\n",
        "print(num_list)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'Year', 'Month', 'Day']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRuBa-G73Ztf",
        "colab_type": "text"
      },
      "source": [
        "### Step 7.2\n",
        "Import Imputer from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xBFT0KX3Ztf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Imputer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGALPfJB3Ztj",
        "colab_type": "text"
      },
      "source": [
        "### Step 7.3\n",
        "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer\">pyspark.ml.feature.imputer</a> to fill in the missing values of the numerical features with mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgrEH01D3Ztj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "double_list = [item[0] for item in data.dtypes if (item[1].startswith('double'))]\n",
        "imputer = Imputer(inputCols = double_list, outputCols = double_list,strategy='mean')\n",
        "data = imputer.fit(data).transform(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjgzwLig3Ztm",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Transform the features (10 marks)\n",
        "### Step 8.1\n",
        "Import skewness from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uwzZH8g3Ztm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import skewness\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvz0O_Ny3Ztp",
        "colab_type": "text"
      },
      "source": [
        "### Step 8.2\n",
        "We can get the skewness using skewness() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module.\n",
        "\n",
        "Find the features which are with skewness values larger than 0.75, and print the features together with their skewness values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfY-4udO3Ztp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9cc52c3e-c6a5-4289-815b-2136bea4dd79"
      },
      "source": [
        "for c in double_list:\n",
        "  num = data.select(skewness(data[c])).collect()[0][0]\n",
        "  if num>0.75:\n",
        "    print(c + '\\t\\t\\t' +str(num))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rainfall\t\t\t9.937207166986028\n",
            "Evaporation\t\t\t4.9535525197618835\n",
            "WindGustSpeed\t\t\t0.9042674275194919\n",
            "WindSpeed9am\t\t\t0.7791876036690152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeGCYtm_3Zts",
        "colab_type": "text"
      },
      "source": [
        "### Step 8.3\n",
        "Import log1p from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXrI5raz3Zts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import log1p\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GugvEJ3Ztw",
        "colab_type": "text"
      },
      "source": [
        "### Step 8.4\n",
        "\n",
        "Apply log transformation on those features with skewness values larger than 0.75 using log1p() from \n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgGnvCsd3Ztw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ske_list = ['Rainfall','Evaporation','WindGustSpeed','WindSpeed9am']\n",
        "\n",
        "for c in ske_list:\n",
        "  data = data.withColumn(c,log1p(data[c]))\n",
        "\n",
        "# a copy of data for pipline later\n",
        "data_pip = data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjz8gdx3Ztz",
        "colab_type": "text"
      },
      "source": [
        "## Step 9 Converting Categorial features  (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nR8rslI3Ztz",
        "colab_type": "text"
      },
      "source": [
        "### Step 9.1 List categorical features\n",
        "Get and print the list of categorial features (exclude \"RainTomorrow\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyTPrdU13Zt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80a0386a-0695-41cf-e148-fd00083fabc4"
      },
      "source": [
        "cat_list = [item[0] for item in data.dtypes if (item[1].startswith('string'))]\n",
        "cat_list.remove('RainTomorrow')\n",
        "print(cat_list)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPQmIlfx3Zt3",
        "colab_type": "text"
      },
      "source": [
        "### Step 9.2 Convert categorical features into dummy/indicator features\n",
        "#### Step 9.2.1\n",
        "Import <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer\">StringIndex</a> and <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoderEstimator\">OneHotEncoderEstimator</a> from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zUrkc4V3Zt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZNAKEw3Zt6",
        "colab_type": "text"
      },
      "source": [
        "#### Step 9.2.2\n",
        "Using StringIndexer to convert categorical values into category indices for each of the categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_-h0PBV3Zt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for c in cat_list:\n",
        "  indexer = StringIndexer(inputCol=c, outputCol=str(c+'_idx'))\n",
        "  data = indexer.fit(data).transform(data)\n",
        "\n",
        "for c in cat_list: \n",
        "  data = data.drop(c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7XSDTkH3Zt8",
        "colab_type": "text"
      },
      "source": [
        "#### Step 9.2.3\n",
        "Using OneHotEncoderEstimator to map a column of category indices to a column of binary vectors for the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4M9p3SX3Zt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_list2 = [item+'_idx' for item in cat_list]\n",
        "\n",
        "encoder = OneHotEncoderEstimator(inputCols=cat_list2,outputCols=cat_list)\n",
        "data = encoder.fit(data).transform(data)\n",
        "\n",
        "for c in cat_list2:\n",
        "  data = data.drop(c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smbadJ4l3ZuA",
        "colab_type": "text"
      },
      "source": [
        "## Step 10: Training the Regression model  (20 points)\n",
        "\n",
        "### Step 10.1: Creating the feature vector\n",
        "\n",
        "#### Step 10.1.1\n",
        "Import VectorAssembler from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIWJTwgu3ZuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUptGpME3ZuD",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.1.2\n",
        "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\">VectorAssembler<a/>, a feature transformer, to merge the columns created in step 9.2.3 and the columns of numerical features into a vector column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yvGDvG3ZuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_list = [item[0] for item in data.dtypes if (item[1].startswith('int'))]\n",
        "f_list = double_list + cat_list + int_list\n",
        "\n",
        "vecAssembler = VectorAssembler(inputCols=f_list, outputCol=\"features\",handleInvalid=\"keep\")\n",
        "data = vecAssembler.transform(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avw1bA473ZuG",
        "colab_type": "text"
      },
      "source": [
        "### Step 10.2 \n",
        "Is there any other transformation on the dataframe needed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCYLI3BP3ZuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert RainTomorrow to boolean label \n",
        "data = data.withColumn('label', when(data['RainTomorrow'] == 'No', lit(0)).otherwise(lit(1)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ2vGfrb3ZuK",
        "colab_type": "text"
      },
      "source": [
        "### Step 10.3 Split the data into training data and testing data\n",
        "\n",
        "Using randomSplit of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
        "to randomly splits the DataFrame with the ratio of 0.7 and 0.3 into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wscpNlmG3ZuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = data.select(['features','label'])\n",
        "train_set, test_set = df.randomSplit([0.7,0.3],0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77xIamIl3ZuP",
        "colab_type": "text"
      },
      "source": [
        "### Step 10.4 Build the logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9g5jSQc3ZuQ",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.4.1\n",
        "Import LogisticRegression from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification\">pyspark.ml.classification</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rPS5OGF3ZuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXEFT2mw3ZuV",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.4.2\n",
        "<ol>\n",
        "<li>Initialize a Logistic Regression model by <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\">LogisticRegression()</a> function using the feature vector generated in Step 10.1.2.</li>\n",
        "<li>Use fit() function to train the logistic regression model using the training feature data.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13_nlYz13ZuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = LogisticRegression(featuresCol='features',labelCol='label').fit(train_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRkCxbNF3ZuY",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.4.3\n",
        "<ul>\n",
        "    <li>Gets summary of model trained on the training set. </li>\n",
        "    <li>Print the accuracy, objective history, total iterations of the trained model.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5sjtaJZ3ZuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f32e8376-62df-4d24-a5ea-eb565e964ca8"
      },
      "source": [
        "clf_info = clf.summary\n",
        "print('summary information:')\n",
        "print('accuracy: ' + str(clf_info.accuracy))\n",
        "print('objective history: ' + str(clf_info.objectiveHistory))\n",
        "print('total iterations: ' + str(clf_info.totalIterations))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "summary information:\n",
            "accuracy: 0.8472351919350142\n",
            "objective history: [0.5319942642901524, 0.531898180666283, 0.531898180666283, 0.5318037511311586, 0.5318000291127355, 0.5317977099598908, 0.5317829746898022, 0.5317528533710458, 0.531665695001189, 0.5314468068658789, 0.5308700638794406, 0.5293923403795086, 0.5256414538007222, 0.5164672166432003, 0.4955449674308512, 0.45617976005284466, 0.4167738536110965, 0.4027853164078687, 0.39682594503489405, 0.3944890298274528, 0.3941063483647683, 0.394069874168949, 0.39406928882900555, 0.3940662639878889, 0.39406032188619383, 0.3940426866348724, 0.3939986244959741, 0.3938819467755806, 0.39358300184676076, 0.39282664828617253, 0.39101995581231747, 0.38723642951595083, 0.38153048026638836, 0.38064428274217305, 0.37626663399632576, 0.3755013238982979, 0.37533635386935693, 0.3753320419571259, 0.37527581321735726, 0.3751659875981029, 0.37484530945495215, 0.37408186988234393, 0.3723104191387503, 0.3690616794802842, 0.36523965545396514, 0.36321654851489665, 0.3629191602576372, 0.3626821167797615, 0.36265610844520096, 0.3626555302876331, 0.3626550105589961, 0.36265305642237905, 0.36264796413360806, 0.3626365728781673, 0.3626058889216622, 0.3625292397294909, 0.3623446658681096, 0.36193401869604097, 0.3617608654374687, 0.3610400914915764, 0.3596199434307119, 0.35903438561816664, 0.35884036502600203, 0.35865633005632613, 0.3584718076775205, 0.35793823123315976, 0.35762777060263384, 0.3574982733876436, 0.3574890676428062, 0.3574888445461874, 0.3574881125216462, 0.35748561727222733, 0.3574815658392532, 0.35746085982589815, 0.35741149927627713, 0.35727639883389845, 0.3569741354015181, 0.35639990504472113, 0.35562958930810323, 0.35560690828273633, 0.35527421851682434, 0.35499366677626404, 0.3549515722904249, 0.3549476144296512, 0.35494732816986563, 0.3549469972725763, 0.35494662523666637, 0.35494568301806395, 0.3549434368836142, 0.35493802503714833, 0.35492709757373164, 0.3549106092338527, 0.35491056802875487, 0.35489719384746815, 0.35489255890517746, 0.3548920837314451, 0.35489199757205486, 0.3548919484751253, 0.35489178165140306, 0.35489133838618164, 0.3548895181086156, 0.3548866570547217]\n",
            "total iterations: 102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59_mjGfr3Zuc",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.4.4\n",
        "Predict the target values for the testing feature data using the transform() function.</li>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlsddGKc3Zuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = clf.transform(test_set)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckAXmm3I3Zuj",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.4.5\n",
        "Use show() to list the top 5 rows of results in Step 10.4.4, show only \"prediction\", \"RainTomorrow\" and the featuresCol specified in the LogisticRegression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVLVKii-3Zuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "99f90471-d14c-4941-bdde-e869942d17ff"
      },
      "source": [
        "pred['prediction','label','features'].head(5)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(prediction=0.0, label=1, features=SparseVector(113, {0: 0.2, 1: 17.0, 2: 0.6931, 3: 1.5686, 4: 7.6249, 5: 3.7132, 6: 2.7082, 7: 18.6376, 8: 81.0, 9: 47.0, 10: 1017.6538, 11: 1015.2582, 12: 4.4372, 13: 4.5032, 14: 8.5, 15: 15.3, 16: 1.0, 64: 1.0, 79: 1.0, 94: 1.0, 109: 1.0, 110: 2009.0, 111: 10.0, 112: 17.0})),\n",
              " Row(prediction=0.0, label=0, features=SparseVector(113, {0: 4.5, 1: 15.5, 2: 0.1823, 3: 0.7885, 4: 7.6249, 5: 3.7132, 6: 2.7082, 7: 18.6376, 8: 78.0, 9: 45.0, 10: 1017.6538, 11: 1015.2582, 12: 4.4372, 13: 4.5032, 14: 9.7, 15: 15.0, 16: 1.0, 64: 1.0, 79: 1.0, 94: 1.0, 109: 1.0, 110: 2009.0, 111: 8.0, 112: 13.0})),\n",
              " Row(prediction=0.0, label=0, features=SparseVector(113, {0: 5.3, 1: 15.3, 2: 0.3365, 3: 1.4351, 4: 7.6249, 5: 3.7132, 6: 2.7082, 7: 18.6376, 8: 46.0, 9: 37.0, 10: 1017.6538, 11: 1015.2582, 12: 4.4372, 13: 4.5032, 14: 10.2, 15: 15.1, 16: 1.0, 64: 1.0, 79: 1.0, 94: 1.0, 109: 1.0, 110: 2009.0, 111: 9.0, 112: 1.0})),\n",
              " Row(prediction=0.0, label=0, features=SparseVector(113, {0: 5.5, 1: 10.9, 2: 0.3365, 3: 1.7579, 4: 7.6249, 5: 3.7132, 6: 2.7082, 7: 18.6376, 8: 60.0, 9: 41.0, 10: 1017.6538, 11: 1015.2582, 12: 4.4372, 13: 4.5032, 14: 6.6, 15: 10.1, 16: 1.0, 64: 1.0, 79: 1.0, 94: 1.0, 109: 1.0, 110: 2009.0, 111: 8.0, 112: 25.0})),\n",
              " Row(prediction=0.0, label=0, features=SparseVector(113, {0: 6.4, 1: 19.5, 2: 0.1823, 3: 1.0986, 4: 7.6249, 5: 3.7132, 6: 2.7082, 7: 18.6376, 8: 69.0, 9: 58.0, 10: 1017.6538, 11: 1015.2582, 12: 4.4372, 13: 4.5032, 14: 10.8, 15: 17.6, 16: 1.0, 64: 1.0, 79: 1.0, 94: 1.0, 109: 1.0, 110: 2009.0, 111: 10.0, 112: 27.0}))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI71Vw8u3Zun",
        "colab_type": "text"
      },
      "source": [
        "### Step 10.5 Evaluate the results\n",
        "#### Step 10.5.1 \n",
        "Import MulticlassClassificationEvaluator from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation\">pyspark.ml.evaluation</a> module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Buopw_p3Zuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-BaxRTn3Zur",
        "colab_type": "text"
      },
      "source": [
        "#### Step 10.5.2\n",
        "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator\">pyspark.ml.evaluation.MulticlassClassificationEvaluator</a> to evaluate the predictions from Step 10.4.4.\n",
        "Print the evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3aa4v7I3Zur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dcd079d-58b4-4263-ccb2-c713a4b0317f"
      },
      "source": [
        "acc = MulticlassClassificationEvaluator(labelCol='label',metricName='accuracy').evaluate(pred)\n",
        "print('accuracy: ' + str(acc))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8435519459180321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrXfV3a3Zuu",
        "colab_type": "text"
      },
      "source": [
        "## Step 11: Pipeline (10 marks)\n",
        "\n",
        "### Step 11.1\n",
        "Import <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline\">Pipeline</a> from \n",
        "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml\">pyspark.ml</a> package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXGoarB43Zuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjBh5cDm3Zuw",
        "colab_type": "text"
      },
      "source": [
        "### Step 11.2\n",
        "\n",
        "Rewrite Steps 9.2 to 10.5:\n",
        "<ol>\n",
        "    <li>Configure an ML pipeline which consists of the StringIndexer, OneHotEncoderEstimator, VectorAssembler, LogisticRegression</li>\n",
        "    <li>Fit the pipeline to the training data.</li>\n",
        "    <li>Make predictions on the testing data.</li>\n",
        "    <li>Evaluate the prediction results as in step 10.5.</li>\n",
        "</ol>\n",
        "\n",
        "Note: \n",
        "<ul>\n",
        "    <li>It is fine to have the steps 9.2 to 10.5 slightly rearranged in this step.</li>\n",
        "    <li>Hence, the evaluation results may be slightly different.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8lvSeiu3Zux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4140835c-5205-4771-e0ef-cd4a1d69834e"
      },
      "source": [
        "cat_list = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
        "cat_list2 = [item+'_idx' for item in cat_list]\n",
        "cat_list3 = [item+'_vec' for item in cat_list]\n",
        "feature_list = ['MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine','WindGustSpeed','WindSpeed9am',\n",
        "                'WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm',\n",
        "                'Temp9am','Temp3pm','Location_vec','WindGustDir_vec','WindDir9am_vec','WindDir3pm_vec',\n",
        "                'RainToday_vec','Year','Month','Day']\n",
        "indexer_list = []\n",
        "\n",
        "# split the data\n",
        "data_pip = data_pip.withColumn('label', when(data_pip['RainTomorrow'] == 'No', lit(0)).otherwise(lit(1)))\n",
        "train_pip, test_pip = data_pip.randomSplit([0.7,0.3],0)\n",
        "\n",
        "# \n",
        "# construct pipline:\n",
        "#\n",
        "\n",
        "# String Indexer\n",
        "for c in cat_list:\n",
        "  indexer_list.append( StringIndexer(inputCol=c, outputCol=str(c+'_idx')) )\n",
        "\n",
        "# One Hot Encoder\n",
        "encoder = OneHotEncoderEstimator(inputCols=cat_list2,outputCols=cat_list3)\n",
        "\n",
        "# Vector Assembler\n",
        "vecAssembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\",handleInvalid=\"keep\")\n",
        "\n",
        "# Logistic Regressor\n",
        "clf = LogisticRegression(featuresCol='features',labelCol='label')\n",
        "\n",
        "pipe = Pipeline(stages = indexer_list + [encoder,vecAssembler,clf])\n",
        "\n",
        "# fit the pipeline\n",
        "pipe_model = pipe.fit(train_pip)\n",
        "\n",
        "# make predictions\n",
        "pipe_pred = pipe_model.transform(test_pip)\n",
        "\n",
        "# evaluate the prediction\n",
        "pipe_acc = MulticlassClassificationEvaluator(labelCol='label',metricName='accuracy').evaluate(pipe_pred)\n",
        "print('accuracy: ' + str(pipe_acc))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8463217689310361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HRWrIuO3Zu5",
        "colab_type": "text"
      },
      "source": [
        "## Step 12 Submission\n",
        "Submit your jupyter notebook (.ipynb) to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7zXRg43Zu6",
        "colab_type": "text"
      },
      "source": [
        "<center>The end of HW2</center>"
      ]
    }
  ]
}